{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8cea32",
   "metadata": {},
   "source": [
    "# Escolha de Modelos e Métricas de Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec310b1",
   "metadata": {},
   "source": [
    "Considerando o contexto da solução proposta, pensando na adequação a metodologia CRISP-DM, temos que as técnicas de NLP aqui empregadas não são aderentes aos moldes de escolha de modelo tradicionais, nem a inserção em técnicas de classificação em regressão. Devido a isso podemos pensar, como podemos avaliar quais modelos estão sendo utilizados em nossa aplicação e como podemos avaliá-los? \n",
    "\n",
    "Mesmo no âmbito de NLP, iremos utilizar pesos de modelos pré-treinados, assim como arquiteturas de Deep Learning, para a geração de Embeddings. Embeddings são representações vetoriais baseando-se nesse treinamento dos modelos prévios que nos devolvem o \"contexto\" a qual trecho de texto se refere, sendo uma técnica essencial para a solução proposta. Com isso iremos iterar sobre diferentes tipos de modelos de Embedding para avaliar qual melhor se adequa ao nosso conjunto de teste.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "A partir da análise exploratória dos dados, vimos que os Requerimentos apresentam muito mais informação textual e são menos propícios a falhas na recuperação que as pautas dos eventos. Devido a isso estes serão nosso foco de análise. O plano agora é verificar como podemos verificar a similaridade de um termo de pesquisa para os documentos, e para isso que iremos utilizar os Embeddings.\n",
    "\n",
    "Em suma aqui discutimos a etapa do projeto posterior ao pré-processamento dos dados.\n",
    "\n",
    "Após o pré-processamento como limpeza e tratamentos atrelados a NLP, devemos criar os embeddings que serão salvos em nosso banco vetorial. Para isso iremos utilizar os modelos de linguagem já prontos (treinados) e aqui avaliados. Este processo está presente no arquivo `src/data_preprocess/feature_engineering.py`.\n",
    "\n",
    "Iremos capturar os vetores semânticos após passagem pela arquitetura de rede neural de cada modelo, resgatando os valores da última camada oculta (que apresenta maior informação semântica) a partir do primeiro Token [CLS] que contém um resumo da informação contextual de cada texto.\n",
    "\n",
    "Esses embeddings capturados serão salvos em um banco vetorial ChromaDB a serem resgatados posteriormente.\n",
    "\n",
    "## Conjunto de Teste\n",
    "\n",
    "Tendo em vista que o problema deste projeto lida com dados não estruturados, para construir um \"split\" de teste para avaliação, foi necessário avaliar manualmente os Requerimentos legislativos e associá-los ao que seria um termo de pesquisa. Para isso foi criado dentro da pasta `data/datasets/evaluation` arquivos fixos que representem nossa ground truth, a serem aplicados nos moldes de avaliação.\n",
    "\n",
    "Dos 60 requerimentos analisados na confecção deste projeto (retirados para o intervalo de uma semana), foram separados 21 destes para serem adequados aos dados de testes, contendo 4 temas colocados como entrada:\n",
    "- \"violência contra a mulher e feminicídio\"\n",
    "- \"temas inerentes ao meio ambiente e crise climática\"\n",
    "- \"políticas públicas de saúde\",\n",
    "- \"estatuto da criança e do adolescente\"\n",
    "\n",
    "\n",
    "## Ciclo de testes de modelos\n",
    "\n",
    "Inicialmente o projeto foi pensado com a utilização do modelo \"bert-base-portuguese-cased\", este é chamado de modelo BERTimbau, sendo baseado no modelo BERT muito utilizado para tarefas de NLP, porém treinado em dados da língua portuguesa. As etapas do código que apresentam a criação de Embeddings estão dispostas nos arquivos `src/data_preprocess/feature_engineering.py` que cria os embeddings a serem salvos em um banco vetorial. E em segundo temos na etapa de predição, para adequar a query enviada e buscar sua correspondência no banco, presente no arquivo `src/data_preprocess/similarity_retriever.py`. Porém se torna necessário antes de definirmos como o melhor modelo para produção, verificarmos outras opções baseando-se em métricas.\n",
    "\n",
    "Para isso iremos iterar sobre as opções:\n",
    "\n",
    "- \"e5-base\": \"intfloat/multilingual-e5-base\", https://huggingface.co/intfloat/multilingual-e5-base\n",
    "- \"bge-m3\": \"BAAI/bge-m3\", https://huggingface.co/BAAI/bge-m3 \n",
    "- \"miniLM\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "- \"portuguese-bert\": \"neuralmind/bert-base-portuguese-cased\", https://huggingface.co/neuralmind/bert-base-portuguese-cased\n",
    "\n",
    "Todas as opções vão ser acessadas e estão disponíveis a partir da biblioteca Transformers disponibilizada pelo Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035f8436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renne\\Documents\\repos\\legislativo-nlp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b8db7",
   "metadata": {},
   "source": [
    "O funcionamento do laço de teste, além das chamadas para as funções de criação do banco vetorial e recuperação estão acopladas no arquivo `src/evaluation` para uma construção modular e voltada para a produtização deste projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8c5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_multiple_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3719701",
   "metadata": {},
   "source": [
    "A função `evaluate_multiple_models` irá ler os arquivos construídos como Golden Dataset contendo nossa ground truth de correspondência e irá iterar sobre os modelos para gerar as métricas, criando diversos bancos vetoriais e chamados as queries contendo os temas propostos.\n",
    "\n",
    "Esta etapa é computacionalmente custosa devido a criação de múltiplos tipos de Embeddings, alguns possuem dimensões de saída um pouco maiores que outros, além de estruturas de camadas para as redes neurais diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbe28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59/59 [00:00<00:00, 3312.42 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:27<00:00,  2.15 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 1032.65 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:57<00:00,  1.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Coleção limpa para ChromaDB 'e5-base'.\n",
      "Ementas: 58 documentos\n",
      "PDFs: 58 documentos\n",
      "\n",
      "Avaliando a Query: 'violência contra a mulher e feminicídio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.61 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.52 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 201.52 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.76 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'temas inerentes ao meio ambiente e crise climática'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 199.68 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.72 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 500.27 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'políticas públicas de saúde'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 381.02 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.72 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 250.17 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'estatuto da criança e do adolescente'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 249.50 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.46 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 199.11 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados de avaliação para e5-base\n",
      "             ementa_baseline  pdf_baseline  ementa_llm  pdf_llm\n",
      "Precision@5            0.550         0.200       0.300    0.150\n",
      "Recall@5               0.503         0.129       0.192    0.094\n",
      "MRR                    1.000         0.300       0.500    0.250\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59/59 [00:00<00:00, 4221.35 examples/s]\n",
      "Map: 100%|██████████| 59/59 [01:45<00:00,  1.78s/ examples]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 1005.82 examples/s]\n",
      "Map: 100%|██████████| 59/59 [03:31<00:00,  3.59s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Coleção limpa para ChromaDB 'bge-m3'.\n",
      "Ementas: 58 documentos\n",
      "PDFs: 58 documentos\n",
      "\n",
      "Avaliando a Query: 'violência contra a mulher e feminicídio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 98.86 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  1.98 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 77.61 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  3.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'temas inerentes ao meio ambiente e crise climática'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 67.00 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  1.76 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 56.69 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'políticas públicas de saúde'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 74.27 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  2.26 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 51.81 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'estatuto da criança e do adolescente'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 67.46 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  2.25 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 138.27 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados de avaliação para bge-m3\n",
      "             ementa_baseline  pdf_baseline  ementa_llm  pdf_llm\n",
      "Precision@5            0.600         0.750       0.300    0.700\n",
      "Recall@5               0.538         0.735       0.211    0.699\n",
      "MRR                    0.833         1.000       0.750    1.000\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59/59 [00:00<00:00, 1981.30 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:05<00:00, 10.39 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 1042.49 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:17<00:00,  3.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Coleção limpa para ChromaDB 'miniLM'.\n",
      "Ementas: 58 documentos\n",
      "PDFs: 58 documentos\n",
      "\n",
      "Avaliando a Query: 'violência contra a mulher e feminicídio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.85 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.98 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 333.23 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'temas inerentes ao meio ambiente e crise climática'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 180.38 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.33 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 181.34 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.76 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'políticas públicas de saúde'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 332.51 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.28 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 330.44 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 34.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'estatuto da criança e do adolescente'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 328.48 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.82 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.37 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 34.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados de avaliação para miniLM\n",
      "             ementa_baseline  pdf_baseline  ementa_llm  pdf_llm\n",
      "Precision@5            0.450         0.450       0.300    0.450\n",
      "Recall@5               0.403         0.515       0.211    0.515\n",
      "MRR                    0.646         0.875       0.750    1.000\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59/59 [00:00<00:00, 3229.25 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:21<00:00,  2.74 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:00<00:00, 1483.28 examples/s]\n",
      "Map: 100%|██████████| 59/59 [00:54<00:00,  1.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Coleção limpa para ChromaDB 'portuguese-bert'.\n",
      "Ementas: 58 documentos\n",
      "PDFs: 58 documentos\n",
      "\n",
      "Avaliando a Query: 'violência contra a mulher e feminicídio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.70 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.12 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 249.87 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'temas inerentes ao meio ambiente e crise climática'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 198.88 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.53 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 116.25 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'políticas públicas de saúde'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 181.73 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.87 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 197.56 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando a Query: 'estatuto da criança e do adolescente'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 208.69 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.62 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 199.98 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados de avaliação para portuguese-bert\n",
      "             ementa_baseline  pdf_baseline  ementa_llm  pdf_llm\n",
      "Precision@5            0.350         0.650       0.200    0.550\n",
      "Recall@5               0.345         0.668       0.134    0.601\n",
      "MRR                    0.708         1.000       0.500    1.000\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_multiple_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d0c76",
   "metadata": {},
   "source": [
    "## Métricas utilizadas\n",
    "\n",
    "Antes de prosseguirmos é interessante explicar brevemente quais as métricas utilizadas nessa avaliação além disso explicarmos ao que corresponde as colunas Baseline VS LLM.\n",
    "\n",
    "### 1. Precision@5: \n",
    "\n",
    "\"Dos 5 primeiros resultados que meu sistema retornou, qual a porcentagem de acertos?\"\n",
    "\n",
    "Mede a qualidade do topo da lista. Penaliza resultados irrelevantes mostrados ao usuário. É a métrica mais importante para medir a satisfação imediata.\n",
    "\n",
    "Uma Precision@5 de 0.75 significa que, em média, 75% dos 5 primeiros resultados mostrados eram relevantes.\n",
    "Uma Precision@5 de 0.20 significa que o usuário teve que olhar 5 resultados para encontrar apenas 1 que era relevante.\n",
    "\n",
    "### 2. Recall@5:\n",
    "\n",
    "\"De todos os documentos que eu deveria ter encontrado, qual a porcentagem que meu sistema encontrou nos 5 primeiros resultados?\"\n",
    "\n",
    "O que ela mede: A capacidade do sistema de encontrar todos os itens relevantes. Penaliza ao deixar de fora documentos importantes. É útil quando o número total de documentos relevantes é pequeno.\n",
    "\n",
    "Uma Recall@5 de 0.735 indica que o sistema foi capaz de encontrar, em média, 73.5% dos documentos que eram considerados corretos para as buscas, isso dentro das 5 primeiras posições.\n",
    "Uma Recall@5 de 0.129 mostra que o sistema está falhando em encontrar a maioria dos documentos relevantes.\n",
    "\n",
    "### 3. MRR (Mean Reciprocal Rank):\n",
    "\n",
    "\"Com que rapidez o usuário encontra o primeiro resultado correto?\"\n",
    "\n",
    "Mede a posição (rank) do primeiro acerto. Recompensa quando colocamos um resultado relevante logo na primeira posição.\n",
    "\n",
    "Um MRR de 1.0 é uma pontuação perfeita. Significa que, em todas as buscas realizadas, o primeiro resultado mostrado foi um dos documentos corretos.\n",
    "Um MRR de 0.5 significaria que, em média, a primeira resposta correta apareceu na segunda posição.\n",
    "Um MRR de 0.3 indica que, em média, o usuário teve que descer até a terceira ou quarta posição para encontrar o primeiro item útil.\n",
    "\n",
    "\n",
    "\n",
    "## LLM-as-a-judge\n",
    "\n",
    "Para o propósito deste trabalho, não gostaria de realizar apenas LLMs que fizessem a avaliação da query de entrada. Porém para adequar as tecnologias mais recentes ao processo de Engenharia de IA, foi utilizados LLMs como juízes das recuperações realizadas. Ou seja esta etapa é realizada após a recuperação inicial, tanto no contexto de avaliação quanto de uso pela API.\n",
    "\n",
    "- As recuperçaões iniciais foram feitas apenas utilizando similaridade de Cosseno entre os embeddings da query realizada e dos documentos tratados e salvos.\n",
    "- Após a recuperação inicial, os textos eram passados para um LLM julgador, verificando se a posição dos itens recuperados era concisa, rerankeando os itens.\n",
    "\n",
    "Com isso evitamos a interferência de um modelo mais custoso na recuperação, esta solução diferencia de RAGs em muito processos, como por exemplo a ausência de chunks e a não geração de respostas aumentadas, mas sim retornos diretos dos documentos e informações mais tabulares (mesmo que de origem não estruturada).\n",
    "\n",
    "Foi utilizado a API da OpenAI para a utilização do GPT 3.5-Turbo. \n",
    "\n",
    "Esta lógica pode ser visualizada no arquivo `src/data_retrieval/llm_judge.py`, Este processo foi um dos mais iterativos do trabalho devido ao processo de prompt engineer necessário para criar uma query que avaliasse os resultados de forma objetiva e correta. Porém ainda é um ponto de melhoria para um melhor filtro e pente fino.\n",
    "\n",
    "A prompt inicial foi:\n",
    "\n",
    "```\n",
    "    Você é um analista legislativo sênior, especialista em interpretar\n",
    "    a relevância de documentos da Câmara dos Deputados. Sua tarefa é\n",
    "    avaliar o quão bem um documento adere a um termo de pesquisa do usuário,\n",
    "    fornecendo uma nota de 0 a 10.\n",
    "\n",
    "    Tente também avaliar se o que é presente no documento está dentro do mesmo\n",
    "    macro tema, ou tem intereseccionalidades com o termo pesquisado.\n",
    "\n",
    "    Critérios de Pontuação:\n",
    "    - 10: Relevância Direta: O documento tem aderência perfeita ao termo pesquisado.\n",
    "    - 7-9: Relevância Alta: O documento trata do tópico principal\n",
    "      que foi pesquisado, ou temáticas inerentes a ele.\n",
    "    - 5-6: Relevância Parcial: O documento menciona o tópico,\n",
    "      mas de forma secundária, e possui foco em outro contexto.\n",
    "    - 2-4: Relevância Baixa: O documento contém poucas ou nenhuma palavras-chave\n",
    "      relativo ao termo pesquisado.\n",
    "    - 0-1: Irrelevante.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Termos de pesquisa do Usuário:\n",
    "    \"{query_text}\"\n",
    "\n",
    "    Texto do Documento:\n",
    "    \"{document_text}\"\n",
    "\n",
    "    Sua Avaliação (responda APENAS em formato JSON com as chaves\n",
    "      \"score\" e \"justificativa\"):\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "A partir delas temos a geração de um Score e uma Justificativa para aquele score. Foi definido um score de corte de 7 pontos, sendo um score de certa forma rígido com a avaliação dos documentos.\n",
    "\n",
    "Devido aos custos envolvidos, além da necessidade de criação de chaves de API, não foram testados outros LLMs nessa etapa, além de outros modelos mais recentes, porém pode ser um ponto de melhoria, possibilitando uma melhor avaliação dos documentos recuperados.\n",
    "\n",
    "O fato do LLM agir apenas no conjunto já primeiramente filtrado reduz a quantidade de tokens enviada para a API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e101535",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999333a2",
   "metadata": {},
   "source": [
    "Como resultado da avaliação temos:\n",
    "\n",
    "| Métrica | Estratégia | `bge-m3` | `portuguese-bert` | `miniLM` | `e5-base` |\n",
    "| :--- | :--- | :---: | :---: | :---: | :---: |\n",
    "| **Precision@5**| Ementa (Baseline) | **0.600** | 0.350 | 0.450 | 0.550 |\n",
    "| | PDF (Baseline) | **0.750** | 0.650 | 0.450 | 0.200 |\n",
    "| | Ementa (LLM) | 0.300 | 0.200 | 0.300 | 0.300 |\n",
    "| | PDF (LLM) | **0.700** | 0.550 | 0.450 | 0.150 |\n",
    "| **Recall@5** | Ementa (Baseline) | **0.538** | 0.345 | 0.403 | 0.503 |\n",
    "| | PDF (Baseline) | **0.735** | 0.668 | 0.515 | 0.129 |\n",
    "| | Ementa (LLM) | **0.211** | 0.134 | **0.211** | 0.192 |\n",
    "| | PDF (LLM) | **0.699** | 0.601 | 0.515 | 0.094 |\n",
    "| **MRR** | Ementa (Baseline) | **0.833** | 0.708 | 0.646 | 1.000 |\n",
    "| | PDF (Baseline) | **1.000** | **1.000** | 0.875 | 0.300 |\n",
    "| | Ementa (LLM) | **0.750** | 0.500 | **0.750** | 0.500 |\n",
    "| | PDF (LLM) | **1.000** | **1.000** | **1.000** | 0.250 |\n",
    "\n",
    "O melhor modelo de embeddings utilizado foi o bge-m3, ele demonstrou um desempenho superior até mesmo comparado ao inicialmente idealizado (BERTimbau).\n",
    "\n",
    "Temos também que o texto completo do requerimento em PDF se sobressai sobre o uso apenas da ementa isso sugere que, para este tipo de documento, a ementa pode ser muito concisa direta, sem muitos detalhes. O texto completo, apesar de ter mais \"ruidos\" e jargões técnicos, fornece um contexto semântico muito melhor para o modelo de embedding, resultando em uma busca mais precisa.\n",
    "\n",
    "\n",
    "Um resultado surpreendente foi que o LLM-as-a-Judge piorou os Resultados em quase todos os cenários e para todos os modelos, as métricas após a intervenção do LLM Juiz (ementa_llm e pdf_llm) foram piores do que as da busca vetorial pura (ementa_baseline e pdf_baseline). Por exemplo para o bge-m3, a Precision@5 caiu de 0.75 para 0.70 após o julgamento. Não é uma queda tão brusca e é passível de identificar qual documento que acarretou essa mudança, talvez possuindo o tema como algo secundário e intermediário, ou com outras temáticas mais importantes sendo discutidas. O Recall@5 também caiu de 0.735 para 0.699.\n",
    "\n",
    "Temos queo s critérios de pontuação podem ser muito rígidos ou este está sofrendo com contexto insuficiente, além do que foi comentado na seção anterior, poderiam ser utilizados modelos mais recentes e melhores para essa avaliação. Podemos também revisar o prompt para uma melhor adequação a problemática.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc-pos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
